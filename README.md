
# CV Advanced Assessment ‚Äì Edge AI Video Analytics System

This repository implements an end-to-end **Edge AI Video Analytics pipeline** for the Dataguess AI FAE (Computer Vision) technical assessment. The goal is to simulate a realistic production-style workflow covering the entire lifecycle of a computer vision model from training to deployment.

## üöÄ Key Features

* **Model Training:** YOLOv8 training on custom data with logging and augmentations.
* **Optimization:** Model export pipeline (PyTorch ‚Üí ONNX ‚Üí TensorRT).
* **Multi-backend Inference:** Unified interface for PyTorch, ONNX Runtime, and TensorRT.
* **Real-time Pipeline:** Detector + tracker fusion.
* **Deployment:** FastAPI REST API for serving detections.
* **Quality Assurance:** Unit tests with `pytest` and basic monitoring utilities.

---

## üìÇ Repository Structure

```text
cv-advanced-assessment/
‚îÇ
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ train.py                 # Training script using Ultralytics YOLO
‚îÇ   ‚îú‚îÄ‚îÄ dataset.yaml             # Dataset configuration
‚îÇ   ‚îî‚îÄ‚îÄ logs/...                 # Training logs and artifacts
‚îÇ
‚îú‚îÄ‚îÄ optimization/
‚îÇ   ‚îú‚îÄ‚îÄ export_to_onnx.py        # PyTorch to ONNX export
‚îÇ   ‚îú‚îÄ‚îÄ build_trt_engine.py      # ONNX to TensorRT engine builder
‚îÇ   ‚îú‚îÄ‚îÄ calibrate_int8.py        # INT8 Calibration logic
‚îÇ   ‚îî‚îÄ‚îÄ benchmarks.py            # Latency and FPS benchmarking
‚îÇ
‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îú‚îÄ‚îÄ detector.py              # Main inference class (Multi-backend)
‚îÇ   ‚îú‚îÄ‚îÄ tracker.py               # Object tracking implementation
‚îÇ   ‚îú‚îÄ‚îÄ video_engine.py          # Video processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ fusion.py                # Fusion utilities (Detection + Tracking)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                 # Pre/Post-processing helpers
‚îÇ
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ server.py                # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py               # Pydantic models
‚îÇ   ‚îî‚îÄ‚îÄ docker/Dockerfile        # Containerization setup
‚îÇ
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                # Custom logging setup
‚îÇ   ‚îú‚îÄ‚îÄ fps_meter.py             # FPS and Latency metering
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py             # Metrics dashboard placeholder
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_inference.py        # Inference sanity checks
‚îÇ   ‚îú‚îÄ‚îÄ test_onnx_shapes.py      # Shape validation tests
‚îÇ   ‚îî‚îÄ‚îÄ test_tracker.py          # Tracker logic tests
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ latest.pt                # PyTorch weights
‚îÇ   ‚îú‚îÄ‚îÄ model.onnx               # Exported ONNX model
‚îÇ   ‚îú‚îÄ‚îÄ model_fp16.engine        # TensorRT FP16 Engine
‚îÇ   ‚îú‚îÄ‚îÄ model_int8.engine        # TensorRT INT8 Engine
‚îÇ   ‚îî‚îÄ‚îÄ calibration.cache        # INT8 calibration cache
‚îÇ
‚îú‚îÄ‚îÄ benchmark_results.json       # Output of benchmark scripts
‚îú‚îÄ‚îÄ README.md                    # Project documentation
‚îî‚îÄ‚îÄ report.pdf                   # Technical report

-----

## üõ†Ô∏è 1. Environment Setup

The project was developed and tested on **Python 3.13 (CPU)**.
*Note: TensorRT / pycuda are not required for the basic PyTorch/ONNX pipeline but are needed to build `.engine` files.*

```bash
# Clone repository
git clone [https://github.com/bekiroruk/cv-advanced-assessment.git](https://github.com/bekiroruk/cv-advanced-assessment.git)
cd cv-advanced-assessment

# Create and activate virtual env (Windows / PowerShell example)
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

-----

## üèãÔ∏è 2. Training

Train a YOLOv8 model on the **COCO8** mini dataset.

```bash
python training/train.py
```

**This script will:**

1.  Download/load COCO8 (if not available).
2.  Train YOLOv8 for 10 epochs on CPU.
3.  Save logs/plots to `training/logs/exp_coco8*/`.
4.  Save weights to `training/logs/exp_coco8*/weights/best.pt`.

**Artifacts to inspect:**

  * `results.png`: Training loss & mAP curves.
  * `confusion_matrix.png`: Per-class performance.
  * `train_batch*.jpg`: Augmentations and predictions.

-----

## üîÑ 3. Export to ONNX

After training, export the best checkpoint to ONNX format.

```bash
python optimization/export_to_onnx.py
```

  * **Output:** `models/model.onnx`
  * The script performs a basic shape check between PyTorch and ONNX outputs to ensure consistency.

-----

## ‚ö° 4. TensorRT Engines (FP16 / INT8) ‚Äì Optional

> **Requirement:** These steps require a GPU + TensorRT + pycuda environment (e.g., NVIDIA Jetson or a CUDA server). They are implemented but were not executed on the CPU-only dev machine.

### 4.1 INT8 Calibration

```bash
python optimization/calibrate_int8.py
```

  * Samples \~50 images from the COCO8 train set.
  * Preprocesses them and feeds an `EntropyCalibrator`.
  * Writes an INT8 calibration cache to `models/calibration.cache`.

### 4.2 Build TensorRT Engines

```bash
python optimization/build_trt_engine.py
```

  * Parses `models/model.onnx`.
  * Builds `models/model_fp16.engine` and `models/model_int8.engine`.
  * **Dynamic Shape Profile:**
      * Min: `(1, 3, 480, 480)`
      * Opt: `(1, 3, 640, 640)`
      * Max: `(4, 3, 1280, 1280)`

-----

## üìä 5. Benchmarks

Measure PyTorch vs. ONNX Runtime performance.

```bash
python optimization/benchmarks.py
```

  * **Results:** Written to `benchmark_results.json`.
  * **Metrics:** Average latency, p50 / p95 latency, and FPS estimates.

-----

## üëÅÔ∏è 6. Inference Engine

The main entry point for generic inference is `inference/detector.py`.

**Example Usage:**

```python
from inference.detector import Detector
import cv2

detector = Detector(
    backend="onnx",                 # Options: "torch", "onnx", "tensorrt"
    model_path="models/model.onnx"
)

img = cv2.imread("path/to/image.jpg")
detections = detector(img)
print(detections)
```

**Components:**

  * **Detector:** Unified preprocessing, postprocessing, and Custom NMS. Supports batching and warm-up runs.
  * **Tracker (`inference/tracker.py`):** Simple IoU-based tracker implementation (`SimpleIOUTracker`).
  * **Fusion (`inference/fusion.py`):** Utilities to fuse detector outputs with tracker IDs.
  * **Video Engine (`inference/video_engine.py`):** Skeleton for real-time video processing.

-----

## üåê 7. FastAPI REST API

Start the API server:

```bash
uvicorn api.server:app --reload --port 8000
```

**Available Endpoints:**

  * `GET /health`: Basic health check.
  * `GET /metrics`: Returns backend name and basic latency / FPS stats.
  * `POST /detect`: Accepts an image file. Returns bounding boxes, class IDs, scores, and inference time.

-----

## üìà 8. Monitoring

Monitoring utilities are located in the `monitoring/` directory:

  * `fps_meter.py`: Rolling FPS & latency meter.
  * `logger.py`: Simple JSON logger.
  * `dashboard.py`: Placeholder for future metrics dashboard integration (e.g., Prometheus/Grafana).

-----

## üß™ 9. Tests

Run unit tests to ensure system stability.

```bash
pytest -q
```

**Test Coverage:**

  * ONNX model shapes & dynamic axes.
  * Basic detector inference sanity checks.
  * Tracker behavior (ID assignment, IoU thresholds).

-----

## üìù 10. Notes & Future Work

  * **Dataset:** Replace the toy COCO8 dataset with a real-world dataset (e.g., VisDrone, UAVDT, or a custom edge dataset).
  * **Hardware:** Run `calibrate_int8.py` and `build_trt_engine.py` on a real TensorRT-enabled edge device.
  * **Monitoring:** Integrate monitoring metrics into a live dashboard.
  * **Tracking:** Upgrade the tracker from simple IoU to **ByteTrack**, **DeepSORT**, or **OC-SORT** for more robust multi-object tracking.

-----

**Author:** Bekir Oruk
**Role:** AI Field Application Engineer (Computer Vision) ‚Äì Candidate

```
```
